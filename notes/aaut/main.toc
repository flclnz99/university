\babel@toc {italian}{}\relax 
\babel@toc {italian}{}\relax 
\contentsline {chapter}{\numberline {1}Cenni sui prerequisiti del corso}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}probabilità, eventi indipendenti, probabilità congiunte e condizionate}{1}{section.1.1}%
\contentsline {section}{\numberline {1.2}media, varianza, deviazione standard}{1}{section.1.2}%
\contentsline {section}{\numberline {1.3}Teorema di Bayes}{1}{section.1.3}%
\contentsline {section}{\numberline {1.4}distribuzioni di probabilità (probability distribution functions)}{1}{section.1.4}%
\contentsline {section}{\numberline {1.5}Funzione Gussiana}{1}{section.1.5}%
\contentsline {section}{\numberline {1.6}processi di Bernoulli}{1}{section.1.6}%
\contentsline {section}{\numberline {1.7}test delle ipotesi}{1}{section.1.7}%
\contentsline {section}{\numberline {1.8}intervalli di confidenza}{1}{section.1.8}%
\contentsline {section}{\numberline {1.9}funzione T-Student}{1}{section.1.9}%
\contentsline {chapter}{\numberline {2}Introduzione}{2}{chapter.2}%
\contentsline {section}{\numberline {2.1}Gli ingredienti del machine learning}{2}{section.2.1}%
\contentsline {paragraph}{Esempio: l'email}{2}{section*.2}%
\contentsline {paragraph}{Machine Learning}{3}{section*.4}%
\contentsline {section}{\numberline {2.2}Task}{3}{section.2.2}%
\contentsline {paragraph}{Overfitting.}{3}{section*.6}%
\contentsline {section}{\numberline {2.3}Modelli}{5}{section.2.3}%
\contentsline {paragraph}{Modelli probabilistici}{5}{section*.9}%
\contentsline {paragraph}{Modelli logici.}{5}{section*.10}%
\contentsline {section}{\numberline {2.4}Features}{6}{section.2.4}%
\contentsline {chapter}{\numberline {3}Task}{7}{chapter.3}%
\contentsline {section}{\numberline {3.1}Classificazione}{7}{section.3.1}%
\contentsline {paragraph}{Classificazione binaria.}{7}{section*.13}%
\contentsline {paragraph}{Valutazione}{8}{section*.15}%
\contentsline {paragraph}{Coverage plot.}{10}{section*.19}%
\contentsline {paragraph}{Proprietà del coverage plot.}{11}{section*.22}%
\contentsline {paragraph}{Roc Plots.}{12}{section*.24}%
\contentsline {paragraph}{Da un singolo decision tree derivano molti classificatori.}{12}{figure.caption.27}%
\contentsline {paragraph}{Ma perchè questo è vero?}{13}{section*.29}%
\contentsline {chapter}{\numberline {4}Scoring and Ranking}{14}{chapter.4}%
\contentsline {section}{\numberline {4.1}Scoring Classifier}{14}{section.4.1}%
\contentsline {paragraph}{Scoring Tree}{14}{section*.31}%
\contentsline {paragraph}{Margini.}{14}{section*.33}%
\contentsline {paragraph}{Loss function.}{15}{section*.34}%
\contentsline {section}{\numberline {4.2}Ranking}{17}{section.4.2}%
\contentsline {section}{\numberline {4.3}Stima delle probabilità}{18}{section.4.3}%
\contentsline {paragraph}{Probability estimation tree.}{18}{section*.43}%
\contentsline {paragraph}{Esempio:}{18}{section*.45}%
\contentsline {paragraph}{}{19}{section*.46}%
\contentsline {paragraph}{Stima delle probabilità empiriche.}{19}{section*.47}%
\contentsline {paragraph}{Correzione di Laplace.}{19}{section*.48}%
\contentsline {chapter}{\numberline {5}Classificatori multiclasse}{20}{chapter.5}%
\contentsline {paragraph}{Tavola di contingenza.}{20}{section*.49}%
\contentsline {section}{\numberline {5.1}Estendere classificatori da binari a multiclasse}{20}{section.5.1}%
\contentsline {paragraph}{Uno contro tutti (non ordinato)}{21}{section*.51}%
\contentsline {paragraph}{Uno contro tutti (ordinato)}{22}{section*.54}%
\contentsline {paragraph}{Uno contro uno (simmetrico)}{22}{section*.55}%
\contentsline {paragraph}{Uno contro uno (asimmetrico)}{22}{section*.56}%
\contentsline {paragraph}{Decoding.}{23}{section*.57}%
\contentsline {paragraph}{Difficoltà nell'applicare gli schemi.}{23}{section*.58}%
